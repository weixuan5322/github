{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling page 1\n",
      "Crawling page 2\n",
      "Crawling page 3\n",
      "Crawling page 4\n",
      "Crawling page 5\n",
      "Crawling page 6\n",
      "Crawling page 7\n",
      "Crawling page 8\n",
      "Crawling page 9\n",
      "Crawling page 10\n"
     ]
    }
   ],
   "source": [
    "for page in range(1,11):\n",
    "    print \"Crawling page {}\".format(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling page 1\n",
      "Crawling page 2\n",
      "Crawling page 3\n",
      "Crawling page 4\n",
      "Crawling page 5\n",
      "Crawling page 6\n",
      "Crawling page 7\n",
      "Crawling page 8\n",
      "Crawling page 9\n",
      "Crawling page 10\n"
     ]
    }
   ],
   "source": [
    "#爬取10頁新聞\n",
    "url=\"http://www.appledaily.com.tw/realtimenews/section/new/\"\n",
    "for page in range(1,11):\n",
    "    print \"Crawling page {}\".format(page)\n",
    "    current_url=url+str(page)  #因為page 是int 物件這裡作強制轉型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests as r\n",
    "def list_crawler(url):\n",
    "    #定一個function作url的清單\n",
    "    #Args:\n",
    "    #        url: The url of news list page \n",
    "    #   Return:\n",
    "    #         article_url_list: list of article urls\n",
    "    article_url_list=[]\n",
    "    headers={'user-agent':'Chrome/52.0.2743.116'}\n",
    "    res = r.get(url,headers=headers)\n",
    "    soup=BeautifulSoup(res.text,'lxml')\n",
    "    news_list =soup.find_all('li',class_='rtddt')#soup.find_all(目標tag,老爸tag是誰 可以不用寫全部rtddt busi even)\n",
    "    for news in news_list:#去跑每一段 li裡的東西\n",
    "        print news.font.text # li裡的<h1>子tag\n",
    "        #print news.h1.text 這物件\n",
    "        print \"http://www.appledaily.com.tw\"+news.a['href']#把完整url印出來\n",
    "        return article_url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英國調查十大熱門跑車　BMW i8節能跑...(9539)\n",
      "http://www.appledaily.com.tw/realtimenews/article/strange/20160828/936713/英國調查十大熱門跑車　BMWi8節能跑車也進榜\n"
     ]
    }
   ],
   "source": [
    "url_list = list_crawler(\"http://www.appledaily.com.tw/realtimenews/section/new/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_article(url):#把url連結到的細部內容值 印出來 -------內文爬蟲\n",
    "    #抓取標題、內容、時間\n",
    "    art_dict = {}\n",
    "    res = r.get(url)\n",
    "    soup =BeautifulSoup(res.text,'lxml')\n",
    "    art_dict['title'] = soup.find(id='h1').text\n",
    "    art_dict['content'] = soup.find(id='summary').text\n",
    "    art_dict['pub_date'] = soup.find('time').text\n",
    "    #     print url  把url字串切割\n",
    "    #     print url.split('/')\n",
    "    #     print url.split('/')[5]\n",
    "    art_dict['category'] = url.split('/')[5]\n",
    "    return art_dict#{'title':'【壹週刊】喔～原來萬花筒是他發明的','content':'內文'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "article_dict = parse_article(\"http://www.appledaily.com.tw/realtimenews/article/sports/20160826/936183/%E7%B6%93%E5%85%B8%E8%B3%BD%E5%85%A8%E8%B3%BD%E7%A8%8B%E5%87%BA%E7%88%90%E3%80%80%E6%9C%80%E7%B5%82%E6%B1%BA%E6%88%B0%E9%81%93%E5%A5%87%E7%90%83%E5%A0%B4\")\n",
    "#測是有沒有錯誤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "經典賽全賽程出爐　最終決戰道奇球場\n",
      "(新增影片)2017年經典賽(World Baseball Classic，簡稱WBC)先前已先確定亞洲部份賽程，大聯盟今天公布完整賽程與場地資訊，準決賽與決賽將在道奇球場舉行，這也是道奇球場繼2009年之後，再度舉行經典賽決賽。影片：2017經典賽決戰將在道奇球場舉行Your browser does not support iframes.台灣隊首輪將在首爾高尺巨蛋與前屆同組的南韓、荷蘭，以及剩一組的資格賽勝出者(推測是巴西)，力拼前兩名晉級下一輪，如果能創造歷史、挺到最後一關，將在台灣時間3月21~23日在道奇球場出賽。連結：2017經典賽PDF檔2017經典賽各區各組賽日阿與場地如下，均為當地日期，A組，3月7~10日，東京巨蛋參賽國：澳洲、中國、古巴、日本B組：3月7~10日，首爾高尺巨蛋參賽國：台灣、南韓、荷蘭、最後一個資格賽名額(9月22~25在紐約布魯克林，由巴西、英國、以色列、巴基斯坦競爭最後一個名額)C組：3月9~12日，邁阿密馬林魚球場參賽國：加拿大、哥倫比亞、多明尼加、美國D組：3月9~12日，墨西哥爪達拉哈市哈利斯科運動場參賽國：義大利、墨西哥、波多黎各、委內瑞拉E組8強賽：3月12~15日，東京巨蛋參賽國：A、B組前兩名球隊F組8強賽：3月14~18日，聖地牙哥PETCO球場參賽國：C、D組前兩名球隊準決賽與決賽：3月20~22日，洛杉磯道奇球場F組第一對E組第二、E組第一對F組第二，勝者爭冠(謝岱穎／綜合報導)出版時間：0915影片更新：1137 \n",
      "2016年08月26日11:37\n",
      "sports\n"
     ]
    }
   ],
   "source": [
    "#print article_dict\n",
    "print article_dict['title']\n",
    "print article_dict['content']\n",
    "print article_dict['pub_date']\n",
    "print article_dict['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling page 1\n",
      "Current url is http://www.appledaily.com.tw/realtimenews/section/new/1\n",
      "段宜康鄙視花蓮人惹議　他反批苗栗智力測驗...(0)\n",
      "http://www.appledaily.com.tw/realtimenews/article/politics/20160828/937544/段宜康鄙視花蓮人惹議　他反批苗栗智力測驗選擇\n",
      "Crawling page 2\n",
      "Current url is http://www.appledaily.com.tw/realtimenews/section/new/2\n",
      "【壹週刊】知道眼霜這3件事　不怕熬夜抓寶...(6974)\n",
      "http://www.appledaily.com.tw/realtimenews/article/nextmag/20160828/937318/【壹週刊】知道眼霜這3件事　不怕熬夜抓寶　\n",
      "Crawling page 3\n",
      "Current url is http://www.appledaily.com.tw/realtimenews/section/new/3\n",
      "櫻花多又有補給站　百貨成雨天抓寶聖地(1796)\n",
      "http://www.appledaily.com.tw/realtimenews/article/life/20160828/937501/櫻花多又有補給站　百貨成雨天抓寶聖地\n",
      "Crawling page 4\n",
      "Current url is http://www.appledaily.com.tw/realtimenews/section/new/4\n",
      "​上廁所排隊爆口角　慶生被人打破頭  (10511)\n",
      "http://www.appledaily.com.tw/realtimenews/article/local/20160828/937461/​上廁所排隊爆口角　慶生被人打破頭\n",
      "Crawling page 5\n",
      "Current url is http://www.appledaily.com.tw/realtimenews/section/new/5\n",
      "適應不良？特戰兵飯店墜樓亡(29113)\n",
      "http://www.appledaily.com.tw/realtimenews/article/local/20160828/937373/適應不良？特戰兵飯店墜樓亡\n",
      "Crawling page 6\n",
      "Current url is http://www.appledaily.com.tw/realtimenews/section/new/6\n",
      "轉機不怕丟行李　達美航空幫行李裝晶片(2035)\n",
      "http://www.appledaily.com.tw/realtimenews/article/international/20160828/937408/轉機不怕丟行李　達美航空幫行李裝晶片\n",
      "Crawling page 7\n",
      "Current url is http://www.appledaily.com.tw/realtimenews/section/new/7\n",
      "​川普又失言　消費死者挨轟(7932)\n",
      "http://www.appledaily.com.tw/realtimenews/article/international/20160828/937374/​川普又失言　消費死者挨轟\n",
      "Crawling page 8\n",
      "Current url is http://www.appledaily.com.tw/realtimenews/section/new/8\n",
      "返港客輪撞上不明物體　近300人等待救援(17432)\n",
      "http://www.appledaily.com.tw/realtimenews/article/international/20160828/937347/返港客輪撞上不明物體　近300人等待救援\n",
      "Crawling page 9\n",
      "Current url is http://www.appledaily.com.tw/realtimenews/section/new/9\n",
      "大狗在健身 小汪汪卻在一旁～～(4031)\n",
      "http://www.appledaily.com.tw/realtimenews/article/animal/20160828/936730/大狗在健身小汪汪卻在一旁～～\n",
      "Crawling page 10\n",
      "Current url is http://www.appledaily.com.tw/realtimenews/section/new/10\n",
      "【蘋果報馬仔】紅襪E-ROD腿傷不明　推...(2422)\n",
      "http://www.appledaily.com.tw/realtimenews/article/sports/20160828/937223/【蘋果報馬仔】紅襪E-ROD腿傷不明　推薦皇家\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n[\\n{'title': 'one', 'content': 'asdfasdfasd', 'pub_date': 'asdfasdfdsf'}, # from parse_article(url)\\n{'title': 'one', 'content': 'asdfasdfasd', 'pub_date': 'asdfasdfdsf'},\\n{'title': 'one', 'content': 'asdfasdfasd', 'pub_date': 'asdfasdfdsf'},\\n{'title': 'one', 'content': 'asdfasdfasd', 'pub_date': 'asdfasdfdsf'},\\n{'title': 'one', 'content': 'asdfasdfasd', 'pub_date': 'asdfasdfdsf'},\\n]\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's crawl 10 pages of Appledaily\n",
    "URL = \"http://www.appledaily.com.tw/realtimenews/section/new/\"#前面url不變,主要換頁用迴圈跑完1~10頁\n",
    "final_list = []\n",
    "for page in range(1,11):\n",
    "    # page is type int\n",
    "    print \"Crawling page {}\".format(page)\n",
    "    current_url = URL + str(page)#\"http://www.appledaily.com.tw/realtimenews/section/new/跑回圈(1~10)\"\n",
    "    print \"Current url is {}\".format(current_url)\n",
    "    article_url_list = list_crawler(current_url) # We get the list of article_urls\n",
    "    for article_url in article_url_list:\n",
    "#         try:\n",
    "            final_list.append(parse_article(article_url))\n",
    "#         except Exception as e:\n",
    "#             print e\n",
    "print final_list\n",
    "\"\"\"\n",
    "[\n",
    "{'title': 'one', 'content': 'asdfasdfasd', 'pub_date': 'asdfasdfdsf'}, # from parse_article(url)\n",
    "{'title': 'one', 'content': 'asdfasdfasd', 'pub_date': 'asdfasdfdsf'},\n",
    "{'title': 'one', 'content': 'asdfasdfasd', 'pub_date': 'asdfasdfdsf'},\n",
    "{'title': 'one', 'content': 'asdfasdfasd', 'pub_date': 'asdfasdfdsf'},\n",
    "{'title': 'one', 'content': 'asdfasdfasd', 'pub_date': 'asdfasdfdsf'},\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
